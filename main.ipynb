{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we predict the usefulness of an amazon review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Amazon.com, products can be reviewed by buyers. To prospective buyers, the reviews provide helpful insights about a product, its possible defficiencies or good points. To amazon, they constitute a formidable selling tool: providing reviews attracts users to the amazon website and thus drives amazon's sells. \n",
    "\n",
    "The more helpful and interesting the reviews, the more prospective buyers will use the Amazon website. Therefore, it is amazon primary interest to detect useful reviews and make them stand-out on the web interface.\n",
    "\n",
    "Currently, Amazon uses a user voting system where buyers and prospective buyers can upvote a review for its usefulness. But this introduces a lag between the moment the review is published and the moment when enough users have voted and the review is promoted.\n",
    "\n",
    "So, can we speed up this process using machine learning? Our project attempts to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the notebook\n",
    "\n",
    "This notebook contains python code to parse a dataset of Amazon reviews using spark and predict the usefulness score using machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we run the code on a cluster for production but on the notebook during development, we need a way to detect which libraries to import. The following snippet does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detects wether we're running inside the dev notebook\n",
    "try:\n",
    "    get_ipython\n",
    "    notebook = True\n",
    "except:\n",
    "    notebook = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes all our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the dataset using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "if notebook:\n",
    "    dataFile = 'sample_us.tsv'\n",
    "else:\n",
    "    dataFile = 'hdfs:///datasets/amazon_multiling/tsv/amazon_reviews_us*tsv.gz'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('marketplace', StringType()),\n",
    "    StructField('customer_id', IntegerType()),\n",
    "    StructField('review_id', StringType()),\n",
    "    StructField('product_id', StringType()),\n",
    "    StructField('product_parent', IntegerType()),\n",
    "    StructField('product_title', StringType()),\n",
    "    StructField('product_category', StringType()),\n",
    "    StructField('star_rating', IntegerType()),\n",
    "    StructField('helpful_votes', IntegerType()),\n",
    "    StructField('total_votes', IntegerType()),\n",
    "    StructField('vine', StringType()),\n",
    "    StructField('verified_purchase', StringType()),\n",
    "    StructField('review_headline', StringType()),\n",
    "    StructField('review_body', StringType()),\n",
    "    StructField('review_date', DateType()),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(dataFile, sep=\"\\t\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data cleaning: let's get rid of incomplete entries right now and duplicate a column in prevision for the machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()\n",
    "df = df.selectExpr(\"helpful_votes as label\", \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reduce the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook:\n",
    "    x_core = 1 # number of reviews a product must have\n",
    "else:\n",
    "    x_core = 5\n",
    "\n",
    "# This query returns the number of products with at least x reviews\n",
    "query1 = '''\n",
    "    SELECT product_id\n",
    "    FROM df\n",
    "    GROUP BY product_id\n",
    "    HAVING COUNT(*) >= %s\n",
    "''' % x_core\n",
    "\n",
    "# This query returns the rows for reviews for products with at least x reviews\n",
    "query2 = '''\n",
    "SELECT *\n",
    "FROM df\n",
    "WHERE product_id IN ({})\n",
    "'''.format(query1)\n",
    "\n",
    "df.registerTempTable(\"df\")\n",
    "df = spark.sql(query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output of `query1` for some values of `x_core`:\n",
    "    \n",
    "- Number of 1-core reviews: 21390118\n",
    "- Number of 2-core reviews: 10213901\n",
    "- Number of 3-core reviews:  6931152\n",
    "- Number of 4-core reviews:  5318037\n",
    "- Number of 5-core reviews:  4342875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use machine-learning to attempt to predict to reviews' `helpful_votes`. First we split the dataset into `train`, `validation` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.90, 0.05, 0.05], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create features from the reviews' text-content using TF-IDF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"words\")\n",
    "hashtf    = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf       = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current model is a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit this pipeline to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, lr])\n",
    "\n",
    "pipeline_fit = pipeline.fit(train_set)\n",
    "train_df = pipeline_fit.transform(train_set)\n",
    "#val_df = pipeline_fit.transform(val_set) #to be used later during cross-validation\n",
    "test_df = pipeline_fit.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how good we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data = 0.664671 and on test data = 0.499935\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "train_rmse = evaluator.evaluate(train_df)\n",
    "test_rmse = evaluator.evaluate(test_df)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on train data = %g and on test data = %g\" % (train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code on the cluster, the ouput is:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE) on train data = 21.1488 and on test data = 21.2182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To improve the predictions, we need to use more complex pipelines:\n",
    "\n",
    "- using Glove embeddings instead of TF-IDF;\n",
    "- using a more flexible model than a linear regression, such as a neural network;\n",
    "- using meta-data such as reviewer's id to attempt to increase accuracy.\n",
    "\n",
    "This new model will be tuned using cross-validation.\n",
    "\n",
    "**Agenda**\n",
    "\n",
    "- *by december 05th:* implement this new pipeline (10 days)\n",
    "- *by december 15th:* write a report with our process and findings about spark and the dataset  (10 days)\n",
    "- then practice oral presentation until presentation date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
