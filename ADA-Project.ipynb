{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Documentation\n",
    "\n",
    "- First, install the EPFL VPN using the instructions from here: https://epnet.epfl.ch/AnyConnect-VPN-Clients\n",
    "\n",
    "- How to run jobs in the cluster: https://drive.google.com/open?id=1n9tIfMkDPW6RDLFPvhhetOIcFCBNMnpfESzHc20MM4w\n",
    "\n",
    "- To install Torch (the ML library) run the following commands on the cluster:\n",
    "\n",
    "```curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py```\n",
    "\n",
    "```python get-pip.py --user```\n",
    "\n",
    "```pip install --user torch torchvision```\n",
    "\n",
    "### Get The Code From the NoteBook To The Cluster:\n",
    "\n",
    "We can write our python script in this notebook, but to run it we will need to scp to the server and run ```spark-submit```\n",
    "\n",
    "```scp script.py GASPARID@iccluster028.iccluster.epfl.ch:/home/GASPARID/script.py```\n",
    "\n",
    "```ssh GASPARID@iccluster028.iccluster.epfl.ch```\n",
    "\n",
    "```spark-submit --master yarn --deploy-mode client --driver-memory 4G --num-executors 5 --executor-memory 4G --executor-cores 5 script.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Don't include when running on the cluster ##########\n",
    "import findspark\n",
    "findspark.init()\n",
    "###############################################################\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "########## Use this when running on cluster ##########\n",
    "# This reads in all files that start with amazon_reviews_us (the english reviews)\n",
    "dataFile = 'hdfs:///datasets/amazon_multiling/tsv/amazon_reviews_us*tsv.gz'\n",
    "############ Use this when running locally ###########\n",
    "#dataFile = 'sample_us.tsv'\n",
    "######################################################\n",
    "\n",
    "# Manually specify the schema\n",
    "schema = StructType([\n",
    "    StructField('marketplace', StringType()),\n",
    "    StructField('customer_id', IntegerType()),\n",
    "    StructField('review_id', StringType()),\n",
    "    StructField('product_id', StringType()),\n",
    "    StructField('product_parent', IntegerType()),\n",
    "    StructField('product_title', StringType()),\n",
    "    StructField('product_category', StringType()),\n",
    "    StructField('star_rating', IntegerType()),\n",
    "    StructField('helpful_votes', IntegerType()),\n",
    "    StructField('total_votes', IntegerType()),\n",
    "    StructField('vine', StringType()),\n",
    "    StructField('verified_purchase', StringType()),\n",
    "    StructField('review_headline', StringType()),\n",
    "    StructField('review_body', StringType()),\n",
    "    StructField('review_date', DateType()),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(dataFile, sep=\"\\t\", header=True, schema=schema)\n",
    "\n",
    "# We will want to take a subset of the data\n",
    "# x-core means that all items have at least x reviews\n",
    "x_core = 1\n",
    "\n",
    "# This query returns the number of products with at least x reviews\n",
    "query1 = '''\n",
    "    SELECT product_id\n",
    "    FROM df\n",
    "    GROUP BY product_id\n",
    "    HAVING COUNT(*) >= %s\n",
    "''' % x_core\n",
    "\n",
    "# This query returns the rows for reviews for products with at least x reviews\n",
    "query2 = '''\n",
    "SELECT *\n",
    "FROM df\n",
    "WHERE product_id IN\n",
    "    (     \n",
    "        SELECT product_id\n",
    "        FROM df\n",
    "        GROUP BY product_id\n",
    "        HAVING COUNT(*) >= %s\n",
    "    )\n",
    "''' % x_core\n",
    "\n",
    "df.registerTempTable(\"df\")\n",
    "df = spark.sql(query2)\n",
    "# Results for all files that start with amazon_reviews_us (number of rows returned by query1):\n",
    "#  Number of 1-core reviews: 21390118\n",
    "#  Number of 2-core reviews: 10213901\n",
    "#  Number of 3-core reviews:  6931152\n",
    "#  Number of 4-core reviews:  5318037\n",
    "#  Number of 5-core reviews:  4342875\n",
    "\n",
    "# Index categorical stringType columns to numerical\n",
    "# This is to help with ML later on\n",
    "# Might want to make more columns numerical later on. I just did the obvious ones\n",
    "categoricalColumns = [\"marketplace\", \"product_category\", \"vine\", \"verified_purchase\"]\n",
    "for col in categoricalColumns:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"Index\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"********** FINISHED **********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
