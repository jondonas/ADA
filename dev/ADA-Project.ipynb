{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Documentation\n",
    "\n",
    "- First, install the EPFL VPN using the instructions from here: https://epnet.epfl.ch/AnyConnect-VPN-Clients\n",
    "\n",
    "- How to run jobs in the cluster: https://drive.google.com/open?id=1n9tIfMkDPW6RDLFPvhhetOIcFCBNMnpfESzHc20MM4w\n",
    "\n",
    "- To install Torch (the ML library) run the following commands on the cluster:\n",
    "\n",
    "```curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py```\n",
    "\n",
    "```python get-pip.py --user```\n",
    "\n",
    "```pip install --user torch torchvision```\n",
    "\n",
    "## Get The Code From the NoteBook To The Cluster:\n",
    "\n",
    "We can write our python script in this notebook, but to run it we will need to scp to the server and run ```spark-submit```\n",
    "\n",
    "```export GASPARID=<your gaspar>```\n",
    "\n",
    "```scp script.py $GASPARID@iccluster028.iccluster.epfl.ch:/home/$GASPARID/script.py```\n",
    "\n",
    "```ssh $GASPARID@iccluster028.iccluster.epfl.ch```\n",
    "\n",
    "```spark-submit --master yarn --deploy-mode client --driver-memory 4G --num-executors 5 --executor-memory 4G --executor-cores 5 script.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script fails after 2 hours of execution with \"not enough memory\".\n",
    "\n",
    "Error:\n",
    "\n",
    "``` org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 110.0 failed 4 times, most recent failure: Lost task 24.3 in stage 110.0 (TID 2831, iccluster039.iccluster.epfl.ch, executor 42): ExecutorLostFailure (executor 42 exited caused by one of the running tasks) Reason: Containerkilled by YARN for exceeding memory limits. 5.3 GB of 5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.```\n",
    "\n",
    "It works if we increase the memory to 20Gb. The ouput is:\n",
    "\n",
    "`Root Mean Squared Error (RMSE) on train data = 21.1488 and on test data = 21.2182`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data = 0.664671 and on test data = 0.278657\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------\n",
    "#Detects wether we're running inside the dev notebook\n",
    "try:\n",
    "    get_ipython\n",
    "    notebook = True\n",
    "except:\n",
    "    notebook = False\n",
    "#----------------------------------------------------------\n",
    "    \n",
    "if notebook:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load data\n",
    "#----------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "if notebook:\n",
    "    dataFile = 'sample_us.tsv'\n",
    "else:\n",
    "    dataFile = 'hdfs:///datasets/amazon_multiling/tsv/amazon_reviews_us*tsv.gz'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('marketplace', StringType()),\n",
    "    StructField('customer_id', IntegerType()),\n",
    "    StructField('review_id', StringType()),\n",
    "    StructField('product_id', StringType()),\n",
    "    StructField('product_parent', IntegerType()),\n",
    "    StructField('product_title', StringType()),\n",
    "    StructField('product_category', StringType()),\n",
    "    StructField('star_rating', IntegerType()),\n",
    "    StructField('helpful_votes', IntegerType()),\n",
    "    StructField('total_votes', IntegerType()),\n",
    "    StructField('vine', StringType()),\n",
    "    StructField('verified_purchase', StringType()),\n",
    "    StructField('review_headline', StringType()),\n",
    "    StructField('review_body', StringType()),\n",
    "    StructField('review_date', DateType()),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(dataFile, sep=\"\\t\", header=True, schema=schema)\n",
    "df = df.na.drop()\n",
    "df = df.selectExpr(\"helpful_votes as label\", \"*\")\n",
    "df.registerTempTable(\"df\")\n",
    "   \n",
    "#----------------------------------------------------------\n",
    "# Reduce dataset size\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# We will want to take a subset of the data\n",
    "# x-core means that all items have at least x reviews\n",
    "x_core = 5\n",
    "\n",
    "# This query returns the number of products with at least x reviews\n",
    "query1 = '''\n",
    "    SELECT product_id\n",
    "    FROM df\n",
    "    GROUP BY product_id\n",
    "    HAVING COUNT(*) >= %s\n",
    "''' % x_core\n",
    "\n",
    "# This query returns the rows for reviews for products with at least x reviews\n",
    "query2 = '''\n",
    "SELECT *\n",
    "FROM df\n",
    "WHERE product_id IN\n",
    "    (     \n",
    "        SELECT product_id\n",
    "        FROM df\n",
    "        GROUP BY product_id\n",
    "        HAVING COUNT(*) >= %s\n",
    "    )\n",
    "''' % x_core\n",
    "\n",
    "df = spark.sql(query2)\n",
    "# Results for all files that start with amazon_reviews_us (number of rows returned by query1):\n",
    "#  Number of 1-core reviews: 21390118\n",
    "#  Number of 2-core reviews: 10213901\n",
    "#  Number of 3-core reviews:  6931152\n",
    "#  Number of 4-core reviews:  5318037\n",
    "#  Number of 5-core reviews:  4342875\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Do machine learning\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Split\n",
    "(train_set, val_set, test_set) = df.randomSplit([0.90, 0.05, 0.05], seed = 0)\n",
    "\n",
    "# Preprocess\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# Select model and fix pipeline\n",
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, lr])\n",
    "\n",
    "# Train and predict\n",
    "pipeline_fit = pipeline.fit(train_set)\n",
    "train_df = pipeline_fit.transform(train_set)\n",
    "val_df = pipeline_fit.transform(val_set)\n",
    "test_df = pipeline_fit.transform(test_set)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "train_rmse = evaluator.evaluate(train_df)\n",
    "test_rmse = evaluator.evaluate(val_df)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on train data = %g and on test data = %g\" % (train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** FINISHED **********\n"
     ]
    }
   ],
   "source": [
    "# We will want to take a subset of the data\n",
    "# x-core means that all items have at least x reviews\n",
    "x_core = 5\n",
    "\n",
    "# This query returns the number of products with at least x reviews\n",
    "query1 = '''\n",
    "    SELECT product_id\n",
    "    FROM df\n",
    "    GROUP BY product_id\n",
    "    HAVING COUNT(*) >= %s\n",
    "''' % x_core\n",
    "\n",
    "# This query returns the rows for reviews for products with at least x reviews\n",
    "query2 = '''\n",
    "SELECT *\n",
    "FROM df\n",
    "WHERE product_id IN\n",
    "    (     \n",
    "        SELECT product_id\n",
    "        FROM df\n",
    "        GROUP BY product_id\n",
    "        HAVING COUNT(*) >= %s\n",
    "    )\n",
    "''' % x_core\n",
    "\n",
    "df2 = spark.sql(query2)\n",
    "# Results for all files that start with amazon_reviews_us (number of rows returned by query1):\n",
    "#  Number of 1-core reviews: 21390118\n",
    "#  Number of 2-core reviews: 10213901\n",
    "#  Number of 3-core reviews:  6931152\n",
    "#  Number of 4-core reviews:  5318037\n",
    "#  Number of 5-core reviews:  4342875\n",
    "\n",
    "# Index categorical stringType columns to numerical\n",
    "# This is to help with ML later on\n",
    "# Might want to make more columns numerical later on. I just did the obvious ones\n",
    "#categoricalColumns = [\"marketplace\", \"product_category\", \"vine\", \"verified_purchase\"]\n",
    "#for col in categoricalColumns:\n",
    "#    indexer = StringIndexer(inputCol=col, outputCol=col + \"Index\")\n",
    "#    df = indexer.fit(df).transform(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"********** FINISHED **********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|label|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
      "+-----+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|    0|         US|     125518|R2MW3TEBPWKENS|B00MZ6BR3Q|     145562057|Monster High Haun...|            Toys|          5|            0|          0|   N|                Y|          Five Stars|Love it great add...| 2015-08-31|\n",
      "|    0|         US|     128540|R24VKWVWUMV3M3|B004S8F7QM|     829220659|Cards Against Hum...|            Toys|          5|            0|          0|   N|                Y|          Five Stars|         Tons of fun| 2015-08-31|\n",
      "|    0|         US|     433677|R2B8VBEPB4YEZ7|B00FGPU7U2|     780517568|Fisher-Price Octo...|            Toys|          5|            0|          0|   N|                Y|          Five Stars|    Children like it| 2015-08-31|\n",
      "|    0|         US|    1297934|R1CB783I7B0U52|B0013OY0S0|     269360126|Claw Climber Goli...|            Toys|          1|            0|          1|   N|                Y|Shame on the sell...|Showed up not how...| 2015-08-31|\n",
      "|    0|         US|    5543658|R23JRQR6VMY4TV|B008AL15M8|     211944547|Yu-Gi-Oh! - Solem...|            Toys|          5|            0|          0|   N|                Y|Absolutely one of...|Absolutely one of...| 2015-08-31|\n",
      "+-----+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "(train_set, val_set, test_set) = df.randomSplit([0.90, 0.05, 0.05], seed = 0)\n",
    "train_set.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+-----+\n",
      "|     review_id|               words|                  tf|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+-----+\n",
      "|R2MW3TEBPWKENS|[love, it, great,...|(65536,[7284,8436...|(65536,[7284,8436...|    0|\n",
      "|R24VKWVWUMV3M3|     [tons, of, fun]|(65536,[8443,9639...|(65536,[8443,9639...|    0|\n",
      "|R2B8VBEPB4YEZ7|[children, like, it]|(65536,[11650,206...|(65536,[11650,206...|    0|\n",
      "|R1CB783I7B0U52|[showed, up, not,...|(65536,[1536,4200...|(65536,[1536,4200...|    0|\n",
      "|R23JRQR6VMY4TV|[absolutely, one,...|(65536,[4427,5660...|(65536,[4427,5660...|    0|\n",
      "+--------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "\n",
    "train_df[['review_id', 'words', 'tf', 'features', 'label']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 34\n",
      "RMSE: 0.664671\n",
      "r2: 0.267563\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "#print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "#trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % lr_model.summary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.278657\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = lr_model.transform(val_df)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
